{\rtf1\ansi\ansicpg1252\cocoartf1671\cocoasubrtf600
{\fonttbl\f0\fnil\fcharset0 Menlo-Regular;\f1\fnil\fcharset0 Menlo-Bold;}
{\colortbl;\red255\green255\blue255;\red0\green0\blue0;}
{\*\expandedcolortbl;;\csgray\c0;}
\paperw11900\paperh16840\margl1440\margr1440\vieww12300\viewh12900\viewkind0
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f0\fs24 \cf0 \

\f1\b\fs28 Curso ML con Python \

\f0\b0\fs24 \

\f1\b\fs26 \'cdndice (notebooks)
\f0\b0\fs24 \
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f1\b\fs22 \cf2 \CocoaLigature0 T01-1-Data Cleaning-Carga de datos\

\f0\b0 	* Carga de datos con la funci\'f3n read_csv()\
	* Carga de datos con la funci\'f3n open()\
	* Lectura y escritura de ficheros\
	* Leer datos desde una url externa\
	* Generar una funci\'f3n propia\
	* Ficheros XLS y XLSX\
\

\f1\b T01-2-Data Cleaning-An\'e1lisis preliminar\

\f0\b0 	* Resumen de los datos: dimensiones y estructura\
	* Resumen de estad\'edsticos b\'e1sicos\
	* Missing Values\
	* Gesti\'f3n de NAs\
	* Variables Dummy\
\

\f1\b T01-3-Data Cleaning-Plots\

\f0\b0 	* Plots y visualizaci\'f3n de datos\
	* Scatter plot\
	* Histogramas\
	* Boxplot\
\

\f1\b T02-1-Data Cleaning-DataWrangling\

\f0\b0 	* Data Wrangling o Cirug\'eda de datos\
	* Crear un subconjunto de datos (con set, bucles, condiciones booleans, filtrado de filas y columnas)\
	* Crear una nueva variable\
	* Generaci\'f3n de n\'fameros pseudo aleatorios\
	* Shuffling\
	* Choice\
	* La semilla aleatoria\
\

\f1\b T02-2-Data Cleaning-Funciones de distribuci\'f3n de probabilidad\

\f0\b0 	* Distribuciones de probabilidad\
	* Distribuci\'f3n uniforme\
	* Distribuci\'f3n normal\
	* Simulaci\'f3n de Monte Carlo\
	* Dummy Data Sets\
\

\f1\b T02-3-DataCleaning-Agrupaci\'f3n de datos\

\f0\b0 	* Agregaci\'f3n por categor\'eda\
	* Agrupaci\'f3n de datos\
	* Operaciones sobre datos agrupados\
	* Filtrado de datos\
	* Transformaci\'f3n de variables\
	* Otras operaciones \'fatiles para el agrupado de datos (head, tail, nth, sort_values, groupby)\
	* Conjunto de entrenamiento y conjunto de testing\
	* Dividir usando la distribuci\'f3n normal\
	* Dividir con la librer\'eda sklearn\
	* Dividir usando shuffle\
	\

\f1\b T02-4-Data Cleaning-Concatenacion de datos\

\f0\b0 	* Concatenar y apedazar datos\
	* Carga de datos distribuidos\
	* Joins de datasets\
	* Tipos de joins\
	* Inner Join\
	* Left Join\
	* Right Join\
	* Outer Join\
\

\f1\b T03-1-Statistics-Correlacion \

\f0\b0 	* Correlaci\'f3n de variables\
	* Matriz de correlaci\'f3n y mapa de calor de correlaci\'f3n\
\

\f1\b T04-0-LinearRegression-Resumen \

\f0\b0 	* Resumen de la lecci\'f3n de Regresi\'f3n lineal\
\

\f1\b T04-1-Linear Regresion-Datos ficticios\

\f0\b0 	* Modelo con datos simulados\
	* Error del modelo\
	* Estimaci\'f3n de par\'e1metros mediante la t\'e9cnica de m\'ednimos cuadrados\
	* Otras maneras de verificar la regresi\'f3n lineal (estad\'edsticos)\
\

\f1\b T04-2-LinearRegresion-DemostracionSST\

\f0\b0 	* Demostraci\'f3n de SST = SSR + SSD\
\

\f1\b T04-3-LinearRegression-RegresionLinealSimple\

\f0\b0 	* Regresi\'f3n lineal simple\
	* El paquete statsmodel para regresi\'f3n lineal\
	* Regresi\'f3n lineal m\'faltiple\
	* Regresi\'f3n lineal m\'faltiple con statsmodel\
	* Multicolinealidad\
\

\f1\b T04-4-LinearRegression-Validaci\'f3n del modelo\

\f0\b0 	* Validaci\'f3n del modelo de regresi\'f3n\
	* Validaci\'f3n con el conjunto de testing\
\

\f1\b T04-5-LinearRegression-Regresi\'f3nSciKit-Learn\

\f0\b0 	* Regresi\'f3n lineal con sklearn\
	* Regresi\'f3n lineal con variables categ\'f3ricas\
	* Eliminar variables dummies redundantes\
	* Transformaci\'f3n de variables para conseguir relaci\'f3n lineal\
	* Modelo de regresi\'f3n cuadr\'e1tico\
	* Modelo de regresi\'f3n lineal y cuadr\'e1tico (combinado)\
	* Problemas al implementar modelos de regresi\'f3n lineal\
	* Aspectos a tener en cuenta al implementar un modelo de regresi\'f3n lineal (an\'e1lisis residuos, varianza)\
	\

\f1\b T05-0-LogisticRegression-Summary\

\f0\b0 	* Resumen de Regresi\'f3n Log\'edstica\
\

\f1\b T05-1-LogisticRegression-Theory\

\f0\b0 	* Las matem\'e1ticas tras la regresi\'f3n log\'edstica\
	* Tablas de contingencia\
	* Probabilidad condicional\
	* Ratio de probabilidades\
	* La regresi\'f3n log\'edstica desde la regresi\'f3n lineal\
	* Regresi\'f3n log\'edstica m\'faltiple\
	* Estimaci\'f3n con el modelo de la m\'e1xima verosimilitud\
	* M\'e9todo de Newton-Raphson\
	* Validaci\'f3n cruzada\
	* Matrices de confusi\'f3n y curvas ROC para validar el modelo de Regresi\'f3n Log\'edstica\
\

\f1\b T05-2-LogisticRegression-Implementacion\

\f0\b0 	* Definir la funci\'f3n de entorno\
	* Calcular las probabilidades para cada observaci\'f3n\
	* Calcular la matriz diagonal W\
	* Obtener la soluci\'f3n de la regresi\'f3n de la funci\'f3n log\'edstica\
	* Comprobaci\'f3n experimental\
	* Regresi\'f3n log\'edstica con la librer\'eda statsmodel\
\

\f1\b T05-3-LogisticRegression-ImplementacionPython\

\f0\b0 	* Regresi\'f3n log\'edstica para predicciones bancarias\
	* EDA\
	* Conversi\'f3n de variables categ\'f3ricas a variables dummies\
	* Selecci\'f3n de rasgos para el modelo\
	* Implementaci\'f3n del modelo con statsmodel.api\
	* Validaci\'f3n del modelo log\'edstico (con subconjuntos training y testing)\
	* Validaci\'f3n cruzada\
	* Matrices de confusi\'f3n y curvas ROC para la validaci\'f3n\
\

\f1\b T06-0-Clustering-Theory\

\f0\b0 	* Los objetivos esenciales del clustering\
	* Las caracter\'edsticas principales de una buena clusterizaci\'f3n\
	* La distancia\
	* La matriz de distancias\
	* Normalizar las distancias\
	* M\'e9todos de enlace\
	* Clustering Jer\'e1rquico\
	* Enlaces simple, completo, promedio, centroide, de Ward\
	* Clustering con K-means\
	* Ajustar los par\'e1metros del clustering\
	* M\'e9todo del codo\
	* Coeficiente de la silueta\
	* Resumen del clustering\
\

\f1\b T06-1-Distancias\

\f0\b0 	* Profundizaci\'f3n en distancias y enlaces\
	* Clustering Jer\'e1rquico\
\

\f1\b T06-2-ClusterJerarquico\

\f0\b0 	* Representaci\'f3n gr\'e1fica de un dendrograia\
	* Truncar el dendrograma\
	* Dendrograma tuneado\
	* Corte autom\'e1tico del dendrograma\
	* M\'e9todo del codo\
	* Visualizaci\'f3n final del clustering\
 \

\f1\b T06-3-Clustering-ClusteringKMeans\

\f0\b0 	* Teor\'eda\
\

\f1\b T06-4-ClusteringCompleto\

\f0\b0 	* Implementaci\'f3n con python\
	* Normalizaci\'f3n de los datos\
	* Clustering jer\'e1rquico con sklearn\
	* Clustering k-means con sklearn\
	\

\f1\b T06-5-MetodoCodoSilueta\

\f0\b0 	* Funci\'f3n para iterar silueta\
	* Representaci\'f3n del codo\
	* Representaci\'f3n del codo normalizado\
\

\f1\b T06-6-Clustering-PropagacionAfinidad\

\f0\b0 	* Reporte sobre la propagaci\'f3n de la afinidad\
\

\f1\b T06-7-Clustering-KMedoids\

\f0\b0 	* Distribuciones en forma de anillo\
	* Algoritmo con K-Means\
	* Algoritmo de los K-medoides\
	* Algoritmo de clustering espectral\
\

\f1\b T07-0-Trees-Theory\

\f0\b0 	* Entrop\'eda\
	* Ganancia de informaci\'f3n\
	* Algoritmos para la generaci\'f3n de \'e1rboles\
	* Algoritmo ID3\
	* Otros algoritmos\
	* \'cdndice de Gini\
	* Reducci\'f3n de la varianza\
	* La poda del \'e1rbol\
	* Los problemas de los \'e1rboles de decisi\'f3n\
	* Variables continuas\
	* Valores faltantes\
	* \'c1rboles de regresi\'f3n\
	* Random forest - Bosques aleatorios\
	* Ventajas\
	* Bolseo - bagging o bootstrap	\
	* Por qu\'e9 funcionan los bosques aleatorios\
\

\f1\b T07-1-Tress-ImplementacionPython\

\f0\b0 	* \'c1rboles de decisi\'f3n con python\
	* Visualizaci\'f3n del \'e1rbol de decisi\'f3n\
	* Validaci\'f3n cruzada para la poda\
	* Bosque aleatorio para clasificaci\'f3n\
	\

\f1\b T07-2-Trees-RegressionTrees\

\f0\b0 	* Creaci\'f3n del \'e1rbol\
	* Predicciones\
	* Validaci\'f3n cruzada\
	* Bosque aleatorio para regresi\'f3n\
\

\f1\b T08-0-SVM-Teoria\

\f0\b0 	* Support Vector Machine\
	* SVM para clasificaci\'f3n\
	* Las matem\'e1ticas\
	* Hiperplano que separa clases con margen d\'e9bil\
	* El truco del Kernel\
\

\f1\b T08-1-SVM-SVCLineal\

\f0\b0 	* Linear Support Vector Classifier\
	* Representaci\'f3n del soporte vectorial en 2D\
	\

\f1\b T08-2-SVMModel\

\f0\b0 	* Maximizaci\'f3n del margen\
	* Creaci\'f3n del modelo SVM\
	* Representaci\'f3n de los vectores de soporte\
	\

\f1\b T08-3-SVM-Kernels\

\f0\b0 	* Identificar fronteras no lineales\
	* El kernel no lineal\
	* Ajustar los par\'e1metros del SVMT08-4-SVM-ReconocimientoFacial\
\

\f1\b T08-5-SVM-RegressionVsClassification.ipynb\

\f0\b0 	* Clasificaci\'f3n de flores Iris\
	* Grid Search Cross Validation\
	* Visualizaciones interactivas para jugar con par\'e1metros del modelo\
\

\f1\b T08-6-SVM-Regresi\'f3n\

\f0\b0 	* Implementaci\'f3n simple en python\
	* Ventajas e inconvenientes de los modelos de SVM\
\

\f1\b T09-0-KNN-Teoria\
	
\f0\b0 * Teor\'eda de K Nearest Neighbors\

\f1\b \
T09-1-KNN-Ejemplo\
	
\f0\b0 * Importaci\'f3n y limpieza del data set\
	* Clasificador de los K vecinos\
	* Clasificaci\'f3n sin limpieza (de datos)\
	* Clasificar nuevos datos
\f1\b \
\
T09-2-KNN-Implementacion\

\f0\b0 	* Creando nuestro propio KNN\
	* Aplicar nuestro KNN al data set de Cancer\
	* Conslusiones sobre los algoritmos de ML\
\

\f1\b T09-3-KNN-Sistemas de recomendaci\'f3n\
	
\f0\b0 * Carga de datos de Movie Lens\
	* An\'e1lisis exploratorio de los \'edtems\
	* Representaci\'f3n en forma matricial\
	* Conjuntos de entrenamiento y validaci\'f3n\
	* Filtro colaborativo basado en usuarios\
	* Filtro colaborativo basado en los k usuarios m\'e1s cercanos\
	* Filtro colaborativo basado en \'cdtems\
	* Filtro colaborativo basado en los k \'edtems m\'e1s cercanos\
	* Conclusiones \
\

\f1\b T10-0-ACP-Teoria\

\f0\b0 	* An\'e1lisis de Componentes Principales\
	* La soluci\'f3n de Pearson\
	* C\'e1lculo de las Componentes Principales\
	* Demostraci\'f3n\
\

\f1\b T10-1-ACP-PasoAPaso (implementaci\'f3n manual)\

\f0\b0 	* An\'e1lisis de Componentes Principales - Paso a paso\
	* Representaci\'f3n de los datos con Plotly\
	* Normalizaci\'f3n de los datos\
	1.-C\'e1lculo de la descomposici\'f3n de valores y vectores propios\
		a)Usando la matriz de covarianzas\
		b)Usando la matriz de correlaciones\
		c)Singular Value Decomposition\
	2.-Las componentes principales\
	3.-Proyectar las variables en el nuevo subespecie vectorial\
	\

\f1\b T10-2-ACP-SKlearn\

\f0\b0 	* ACP con Sklearn\
\

\f1\b T10-3-ACP-Plotly\

\f0\b0 	* Dibujar con Plotly\
	* Scatter Plots sencillos\
	* Gr\'e1ficos combinados\
	* Estilizado de gr\'e1ficos con Plotly\
	* Informaci\'f3n al hacer Hover\
	* Data Sets muy grandes\
\

\f1\b T11-0-RN-DeepLearningAprendizajeNoSupervisado\

\f0\b0 	* Deep Learning y Aprendizaje No Supervisado - Teor\'eda\

\f1\b \
T11-1-RN-TensorFlow101\

\f0\b0 	* Introducci\'f3n a TensorFlow\
	\

\f1\b T11-2-RN-Se\'f1alesTrafico (TensorFlow v. 1.X)\
	
\f0\b0 * Aprendizaje neuronal de las se\'f1ales de tr\'e1fico\
	* Carga de datos\
	* An\'e1lisis Exploratorio de Datos\
	* Pre-procesamiento de los datos\
	
\fs24 * Modelo de Red Neuronal con TensorFlow\
	* Ejecuci\'f3n de la Red Neuronal\
	* Evaluaci\'f3n de la Red Neuronal\
	* Validaci\'f3n del modelo
\fs22  
\f1\b \

\f0\b0 \

\f1\b T11-3-RNN-ReconocimientoRopa (TensorFlow v. 2.X)
\f0\b0 \
	* El data set de MNIST\
	* Importar TensorFlow\
	* Carga de datos\
	* Exploraci\'f3n de los datos\
	* Procesamiento de los datos\
	* Normalizar los datos\
	* Construir el modelo\
	* Preparar las capas\
	* Compilar el modelo\
	* Entrenar el modelo\
	* Evaluar la precisi\'f3n\
	* Predicciones\
\

\f1\b T16-0-Python y R\

\f0\b0 	* Combinando R y Python\
	* Acceder a python desde R\
	* Trabajar de manera conjunta entre R y Python\
	* Funci\'f3n m\'e1gica para R en notebook\
	* Ejemplo complejo de R, Python y Rmagic}